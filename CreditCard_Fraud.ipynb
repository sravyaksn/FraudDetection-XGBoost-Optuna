{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4440c77-749c-4820-b7d2-19ce13e0ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 36392, number of negative: 181960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 218352, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.166667 -> initscore=-1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Number of positive: 36392, number of negative: 181961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 218353, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.166666 -> initscore=-1.609443\n",
      "[LightGBM] [Info] Start training from score -1.609443\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 36392, number of negative: 181961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 218353, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.166666 -> initscore=-1.609443\n",
      "[LightGBM] [Info] Start training from score -1.609443\n",
      "[LightGBM] [Info] Number of positive: 36392, number of negative: 181961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 218353, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.166666 -> initscore=-1.609443\n",
      "[LightGBM] [Info] Start training from score -1.609443\n",
      "[LightGBM] [Info] Number of positive: 36392, number of negative: 181961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 218353, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.166666 -> initscore=-1.609443\n",
      "[LightGBM] [Info] Start training from score -1.609443\n",
      "LightGBM - Cross Validation AUC: 1.0000, Test AUC: 0.9763\n",
      "XGBoost - Cross Validation AUC: 1.0000, Test AUC: 0.9799\n",
      "XGBoost is the better performing model for your dataset!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"creditcard.csv\")  # Change the path if necessary\n",
    "\n",
    "# Handling class imbalance\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE for oversampling fraud cases\n",
    "sm = SMOTE(sampling_strategy=0.2, random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Model training function\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred = model.predict_proba(X_fold_val)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_fold_val, y_pred))\n",
    "\n",
    "    final_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, final_pred)\n",
    "\n",
    "    return np.mean(auc_scores), test_auc\n",
    "\n",
    "# Define models\n",
    "lightgbm_model = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=5, num_leaves=20, random_state=42)\n",
    "xgboost_model = xgb.XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "# Evaluate models\n",
    "lightgbm_auc, lightgbm_test_auc = train_evaluate_model(lightgbm_model, X_train, y_train, X_test, y_test)\n",
    "xgboost_auc, xgboost_test_auc = train_evaluate_model(xgboost_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Compare results\n",
    "print(f\"LightGBM - Cross Validation AUC: {lightgbm_auc:.4f}, Test AUC: {lightgbm_test_auc:.4f}\")\n",
    "print(f\"XGBoost - Cross Validation AUC: {xgboost_auc:.4f}, Test AUC: {xgboost_test_auc:.4f}\")\n",
    "\n",
    "if lightgbm_test_auc > xgboost_test_auc:\n",
    "    print(\"LightGBM is the better performing model for your dataset!\")\n",
    "else:\n",
    "    print(\"XGBoost is the better performing model for your dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b76331-e424-4fec-8bb9-f29a4adbbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (1.16.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\vishnu\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9efd7c8-de02-4384-8a3d-b902aa8a1d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 21:36:09,985] A new study created in memory with name: no-name-c46643ad-4989-4e09-8a0a-e3124575e4f7\n",
      "[I 2025-06-07 21:40:44,111] Trial 0 finished with value: 0.9999866199678555 and parameters: {'n_estimators': 336, 'learning_rate': 0.16260541270392093, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.9194544940339705, 'subsample': 0.8786853874641749, 'colsample_bytree': 0.5168560035258449, 'scale_pos_weight': 9.87716406012022}. Best is trial 0 with value: 0.9999866199678555.\n",
      "[I 2025-06-07 21:42:10,030] Trial 3 finished with value: 0.9999903491802445 and parameters: {'n_estimators': 455, 'learning_rate': 0.16646434773107632, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.46784095694736394, 'subsample': 0.6052075468703377, 'colsample_bytree': 0.9633283300975456, 'scale_pos_weight': 5.810324762272433}. Best is trial 3 with value: 0.9999903491802445.\n",
      "[I 2025-06-07 22:10:59,682] Trial 2 finished with value: 0.9999876087069772 and parameters: {'n_estimators': 321, 'learning_rate': 0.09502620621443676, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.36982363105153215, 'subsample': 0.7987141358450752, 'colsample_bytree': 0.6668712771663001, 'scale_pos_weight': 1.3235330765433893}. Best is trial 3 with value: 0.9999903491802445.\n",
      "[I 2025-06-07 22:28:43,806] Trial 1 finished with value: 0.999990995024652 and parameters: {'n_estimators': 484, 'learning_rate': 0.05822087963572027, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 0.15973674287963802, 'subsample': 0.5167317011351986, 'colsample_bytree': 0.8111042366199557, 'scale_pos_weight': 2.6877413008522075}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:31:14,120] Trial 5 finished with value: 0.999726457648924 and parameters: {'n_estimators': 364, 'learning_rate': 0.013768229004803463, 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.004744819313388771, 'subsample': 0.9679325423569791, 'colsample_bytree': 0.5590375960650433, 'scale_pos_weight': 3.153347223855764}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:31:47,173] Trial 4 finished with value: 0.9999908949870051 and parameters: {'n_estimators': 482, 'learning_rate': 0.10923962258537862, 'max_depth': 6, 'min_child_weight': 8, 'gamma': 0.15360705770393124, 'subsample': 0.8307568602076477, 'colsample_bytree': 0.5411398014713651, 'scale_pos_weight': 7.046993734148714}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:46:51,636] Trial 7 finished with value: 0.9999824160099802 and parameters: {'n_estimators': 459, 'learning_rate': 0.17178859653707437, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 0.8093746462361415, 'subsample': 0.9087835103790967, 'colsample_bytree': 0.9414842224821129, 'scale_pos_weight': 4.432150224790442}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:48:34,360] Trial 6 finished with value: 0.9999268570000407 and parameters: {'n_estimators': 408, 'learning_rate': 0.014023084194090409, 'max_depth': 8, 'min_child_weight': 2, 'gamma': 0.8770775041915778, 'subsample': 0.8862709702693632, 'colsample_bytree': 0.911905748022463, 'scale_pos_weight': 5.218795146236587}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:49:38,707] Trial 8 finished with value: 0.9999747138195965 and parameters: {'n_estimators': 352, 'learning_rate': 0.047033833340502366, 'max_depth': 7, 'min_child_weight': 10, 'gamma': 0.3513752717254104, 'subsample': 0.6987706839750144, 'colsample_bytree': 0.994926227763365, 'scale_pos_weight': 1.2357892140668905}. Best is trial 1 with value: 0.999990995024652.\n",
      "[I 2025-06-07 22:49:46,664] Trial 9 finished with value: 0.9999913598742539 and parameters: {'n_estimators': 308, 'learning_rate': 0.13159978349323284, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.13450040016036358, 'subsample': 0.9308347288768503, 'colsample_bytree': 0.8146521731009645, 'scale_pos_weight': 4.725115508465569}. Best is trial 9 with value: 0.9999913598742539.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'n_estimators': 308, 'learning_rate': 0.13159978349323284, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.13450040016036358, 'subsample': 0.9308347288768503, 'colsample_bytree': 0.8146521731009645, 'scale_pos_weight': 4.725115508465569}\n",
      "Optimized XGBoost Test AUC: 0.9804\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 10.0),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        y_pred = model.predict_proba(X_fold_val)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_fold_val, y_pred))\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=10, n_jobs=4)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Train with best parameters\n",
    "optimized_xgb = xgb.XGBClassifier(**best_params)\n",
    "optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = optimized_xgb.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Optimized XGBoost Test AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e7c1ef-fe92-4143-9697-d4b2fd050625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 22:58:40,949] A new study created in memory with name: no-name-9e5cad83-a3c3-4931-a584-5c59aa07225f\n",
      "[I 2025-06-07 23:47:31,330] Trial 2 finished with value: 0.99999069880091 and parameters: {'n_estimators': 304, 'learning_rate': 0.12726072469401534, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 0.3397176248268663, 'subsample': 0.9046360975443829, 'colsample_bytree': 0.8187897810713769, 'scale_pos_weight': 4.472811379047009}. Best is trial 2 with value: 0.99999069880091.\n",
      "[I 2025-06-07 23:47:37,098] Trial 1 finished with value: 0.9999904122333476 and parameters: {'n_estimators': 351, 'learning_rate': 0.1401322464202462, 'max_depth': 8, 'min_child_weight': 4, 'gamma': 0.32500252467661284, 'subsample': 0.8223641657579351, 'colsample_bytree': 0.8656222288345152, 'scale_pos_weight': 4.65751002668649}. Best is trial 2 with value: 0.99999069880091.\n",
      "[I 2025-06-07 23:48:25,092] Trial 0 finished with value: 0.9999910660458088 and parameters: {'n_estimators': 309, 'learning_rate': 0.10283836969834265, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.46119615768953404, 'subsample': 0.9551148836548555, 'colsample_bytree': 0.7901554167916143, 'scale_pos_weight': 7.498382702509613}. Best is trial 0 with value: 0.9999910660458088.\n",
      "[I 2025-06-07 23:49:48,149] Trial 3 finished with value: 0.9999898714871189 and parameters: {'n_estimators': 328, 'learning_rate': 0.08175455536339178, 'max_depth': 10, 'min_child_weight': 6, 'gamma': 0.39189536124708313, 'subsample': 0.9869684011073611, 'colsample_bytree': 0.8508077785897744, 'scale_pos_weight': 4.110136735862077}. Best is trial 0 with value: 0.9999910660458088.\n",
      "[I 2025-06-07 23:54:58,132] Trial 4 finished with value: 0.9999901019830897 and parameters: {'n_estimators': 307, 'learning_rate': 0.11587071940363237, 'max_depth': 7, 'min_child_weight': 4, 'gamma': 0.13850025093860327, 'subsample': 0.83130567653986, 'colsample_bytree': 0.8392909315002577, 'scale_pos_weight': 4.037484045252198}. Best is trial 0 with value: 0.9999910660458088.\n",
      "[I 2025-06-07 23:55:54,601] Trial 5 finished with value: 0.9999920485125606 and parameters: {'n_estimators': 315, 'learning_rate': 0.1293347399717354, 'max_depth': 8, 'min_child_weight': 6, 'gamma': 0.1620729320650899, 'subsample': 0.9341307015017964, 'colsample_bytree': 0.7729095319340881, 'scale_pos_weight': 5.766386479576636}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-07 23:59:41,163] Trial 6 finished with value: 0.999989905554538 and parameters: {'n_estimators': 342, 'learning_rate': 0.07392463347901722, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.2904507089043641, 'subsample': 0.8194483543422, 'colsample_bytree': 0.889519088185462, 'scale_pos_weight': 7.267394030603317}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:00:14,857] Trial 7 finished with value: 0.9999906173674047 and parameters: {'n_estimators': 373, 'learning_rate': 0.12690603764934377, 'max_depth': 9, 'min_child_weight': 7, 'gamma': 0.12439300832737676, 'subsample': 0.8663311635914305, 'colsample_bytree': 0.7705645166828132, 'scale_pos_weight': 4.829168522111223}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:04:39,265] Trial 8 finished with value: 0.9999859801593949 and parameters: {'n_estimators': 370, 'learning_rate': 0.052236244363834165, 'max_depth': 7, 'min_child_weight': 4, 'gamma': 0.23193685108779127, 'subsample': 0.8593345502073632, 'colsample_bytree': 0.8106668121257042, 'scale_pos_weight': 4.9897555586191755}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:04:44,664] Trial 9 finished with value: 0.9999906821228152 and parameters: {'n_estimators': 307, 'learning_rate': 0.12610853783669937, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.392413338352366, 'subsample': 0.8505528144393555, 'colsample_bytree': 0.7253169943954189, 'scale_pos_weight': 7.660312398193025}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:06:35,018] Trial 10 finished with value: 0.9999896351797852 and parameters: {'n_estimators': 322, 'learning_rate': 0.12803403214712394, 'max_depth': 7, 'min_child_weight': 7, 'gamma': 0.45219134150787743, 'subsample': 0.9848860809425912, 'colsample_bytree': 0.73788714624565, 'scale_pos_weight': 4.495333144759981}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:06:52,451] Trial 11 finished with value: 0.9999912187646303 and parameters: {'n_estimators': 265, 'learning_rate': 0.1453777332822604, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.41227452470301484, 'subsample': 0.931176075137817, 'colsample_bytree': 0.7378043708724658, 'scale_pos_weight': 7.169595825098371}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:09:57,201] Trial 12 finished with value: 0.9999898417882143 and parameters: {'n_estimators': 356, 'learning_rate': 0.14312876423181303, 'max_depth': 7, 'min_child_weight': 4, 'gamma': 0.45616266696183083, 'subsample': 0.971212207767153, 'colsample_bytree': 0.7398894822561498, 'scale_pos_weight': 6.653556788099407}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:10:27,674] Trial 13 finished with value: 0.9999907802150444 and parameters: {'n_estimators': 261, 'learning_rate': 0.1432904478141364, 'max_depth': 8, 'min_child_weight': 8, 'gamma': 0.2107221393591734, 'subsample': 0.9190945609699598, 'colsample_bytree': 0.7378685352106552, 'scale_pos_weight': 6.053509638800119}. Best is trial 5 with value: 0.9999920485125606.\n",
      "[I 2025-06-08 00:11:02,509] Trial 14 finished with value: 0.9999906504714658 and parameters: {'n_estimators': 264, 'learning_rate': 0.09771293627679799, 'max_depth': 9, 'min_child_weight': 6, 'gamma': 0.49747349158138265, 'subsample': 0.9420299915977869, 'colsample_bytree': 0.7731484893829853, 'scale_pos_weight': 6.276376845166883}. Best is trial 5 with value: 0.9999920485125606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameters: {'n_estimators': 315, 'learning_rate': 0.1293347399717354, 'max_depth': 8, 'min_child_weight': 6, 'gamma': 0.1620729320650899, 'subsample': 0.9341307015017964, 'colsample_bytree': 0.7729095319340881, 'scale_pos_weight': 5.766386479576636}\n",
      "Final Optimized XGBoost Test AUC: 0.9818\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 250, 400),  # Smaller search range for efficiency\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.15),  # Avoid extreme values\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 7, 10),  # Test deeper trees for fraud detection\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 4, 8),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 0.5),  # Moderate regularization\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.8, 1.0),  # Limit to high values for stability\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.9),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 4.0, 8.0),  # Fine-tuning imbalance handling\n",
    "        \"tree_method\": \"hist\",  # Optimized for speed\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    # Stratified k-fold validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params, early_stopping_rounds=30)\n",
    "        model.fit(X_fold_train, y_fold_train, eval_set=[(X_fold_val, y_fold_val)], verbose=False)\n",
    "\n",
    "        y_pred = model.predict_proba(X_fold_val)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_fold_val, y_pred))\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Run Optuna tuning with pruning\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=15, n_jobs=4)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Optimized hyperparameters:\", best_params)\n",
    "\n",
    "# Train final optimized model\n",
    "optimized_xgb = xgb.XGBClassifier(**best_params)\n",
    "optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = optimized_xgb.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Optimized XGBoost Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc9ca7-6223-46f9-b5c7-eea38bd2634d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa32e13-8372-439c-9906-04e1b85dfa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc352a-6c64-4165-bca7-dc13e6493b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb91fe-b104-44c4-a354-f68570ff69a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
